
Poor reproducibility (the inability of reproducing, by using the original data, the original data analysis exactly) and replicability (the inability of reproducing, by new experiments, the original data approximately) have become major concerns in experimental biomedical science.
So far reproducibility and replicability issues have been mostly addressed in different studies, making it difficult to prise apart their relative contributions to the quality of an experimental system in the wild {Iqbal:2016es} REFS. 

> Determining the reproducibility of a study is relatively straightforward – one needs to ascertain that the original data analysis methods are described in sufficient detail and that the original data is obtainable in an analysable form.

Reproducibility of a study merely suggest that the data analysis of the original study was done, if not correctly, at least successfully by following the published plan.
Reproducibility is a minimum requirement for soundness of a study, but does not guarantee it {Bollen:2015wf}.   
Neither does a successful replication generate new evidence in favour or a scientific hypothesis {Goodman:2016bo}. 

In contrast, a replication study generates new data by the original experimental procedures (although not necessarily the original data analytic methods). Replicability presupposes that we have access to a sufficiently detailed description of the experimental aspects of the study. Unlike a successful reproduction of a study, a successful replication introduces new evidence to science and thus essentially constitutes a form of meta-analysis, which has the potential to change the conclusions of the original study {Goodman:2016bo}. A successful replication is no guarantee for stability of conclusions – indeed, studies that give negative results individually can point to scientifically meaningful effects, when analysed together {Goodman:2016bo}. Moreover, even when presented with identical data there is room for reasonable disagreements about scientific conclusions among people with different interpretations of the scientific context of the problem {Maivali:2015wr}. 
There are two major hurdles in the replication business: the high cost of actually repeating experiments and a lack of consensus on how to measure replicability, which has led to the presentation of several potentially contradictory measures of replicability, even in the same paper (REF). This makes it desirable to first estimate in silico, which studies are worth replicating in the lab, and how to best replicate them. 
To do this efficiently, we will concentrate on a single experimental system: differential expression profiling by RNA sequencing. RNAseq has several advantages as a model system for studying experimental quality. It is currently a widely used and fast-growing system, suggesting that many new researchers adopt it every year ….FIGure DYNAMICS OF PUBLISHING…. With a typical experiment costing in the range of … it consumes a great deal of public funding. There exist formal minimum requirements about describing and depositing high-throughput SEQuencing Experiments (REF) and a single public repository, GEO, that collects most of the deposited RNAseq data in a common, machine-analysable format (REF). RNAseq is usually done in a massively parallel way, testing for differential expression between experiment and control conditions of thousands of RNA pairs. This allows to check the quality of each RNAseq experiment by examining the distribution of the p values that were calculated by the original authors and were used by them to make inferences about differential expression of individual RNAs in their study (REF). Equally importantly, when these p values indicate a technically successful experiment, they can be further used to calculate the false discovery rate (FDR) and the stable retrospective power (SRP), which together enable to analyse the replication potential of the study (REF). 
This leads to a quality control procedure that works by sequential elimination and first discards the studies that do not describe their methods or deposit data, then the ones whose data (the p values) indicate serious problems with experimental design and/or data analysis, and finally the studies, whose low power suggests scientific irrelevance. Thus we aim to estimate the general quality of the RNAseq field, determine the quality of individual studies, estimate the efficiency of potential replication studies, propose improvements in the design of these replication studies, and determine, which step of the RNAseq process would have the greatest gains in terms of the overall health of the field, should improvements be targeted to it. 

We believe that the health of a field of science can be best and cheapest gauged by a sequential elimination procedure: (1) estimating the fraction of reproducible publications in it, (2) actually reproducing its analyses, where possible, and checking for signs of failed experimental design and data analyses, (3) from the publications that by reproducibility criteria are worthy of replication attempts, estimating the statistical power of the experimental systems to provide reproducible results in replication studies, as well as the number of true effects left undiscovered in the original study that could be found in the replication study, (4) estimating the magnitude of changes to the experimental protocol (especially to sample size) that would allow high-powered replication studies.   
 
Here we offer an integrated picture of the reproducibility and expected replicability of a modern and popular method of determining differential expression levels of RNAs, namely expression profiling by RNA sequencing. 
